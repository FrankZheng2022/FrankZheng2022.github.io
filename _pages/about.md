---
layout: about
title: about
permalink: /

profile:
  align: right
  image: Me.jpeg
  image_circular: false # crops the image to make it circular
  more_info: >

news: false # includes a list of news items
latest_posts: false # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a second-year Ph.D. student in Computer Science at the [University of Maryland, College Park](http://www.umd.edu), where I am fortunate to be advised by [Prof. Furong Huang](http://furong-huang.com) and [Prof. Hal Daum√© III](http://users.umiacs.umd.edu/~hal). 
Before that, I obtained my Bachelor's degree in Computer Science and Mathematics both with high honors from the University of Maryland, College Park.
My research spans a variety of topics in sequential decision making and reinforcement learning (RL), including multitask offline pretraining (foundational model for sequential decision making), self-supervised representation learning in visual RL, model-based RL, adversarial RL, etc. 
My long-term goal is to develop a generally capable, robust, and self-adaptive embodied agent, endowed with extensive prior knowledge from a broad spectrum of structured and unstructured data.
You can find my CV [here](http://frankzheng2022.github.io/CV.pdf) and my research statement [here](http://frankzheng2022.github.io/Research_Statement.pdf).

In visual RL, I developed a temporal contrastive representation learning mechanism, [TACO](https://ruijiezheng.com/project/TACO/index.html) that simultaneously learn state and action representations for online and offline visual RL algorithms. Building on top of TACO, [Premier-TACO](https://premiertaco.github.io) scales up to large-scale multitask offline pretraining, learning a universal visual representation for efficient adaptation to new tasks with few-shot imitation learning. Additionally, another of my recent work [DrM](https://drm-rl.github.io) pioneers the first visual RL algorithm mastering a diverse range of complex locomotion and manipulation tasks through the concept of dormant ratio. 

Beyond visuo-motor policy learning, I have also worked on [model-based RL](http://FrankZheng2022.github.io/project/mbrl_lipschitz/index.html), [transfer-RL across different observation spaces](http://FrankZheng2022.github.io/project/transfer/index.html), and [adversarial RL](http://frankzheng2022.github.io/project/evasion-rl/index.html) to make policy robust against observation and communication attacks.


